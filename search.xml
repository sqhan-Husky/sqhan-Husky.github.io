<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>FirstArticle</title>
    <url>/2020/08/03/FirstArticle/</url>
    <content><![CDATA[<p>Welcome to Siqi’s Blog.</p>
<p>I’m READY.</p>
]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读_A Survey of Machine Learning for Big Code and Naturalness</title>
    <url>/2020/08/05/Big-code-survey/</url>
    <content><![CDATA[<blockquote>
<p>NLP技术发展至今，在代码领域的挖掘也逐步深入，近期针对Code Naturalness阅读了一些论文和技术，于是想从这篇比较全面的<a href="https://arxiv.org/abs/1709.06182v1">survey</a>入手，梳理一下整个代码处理的发展史，形成一个整体概念。本文偏向于提纲式的整理。</p>
</blockquote>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p><strong>Objective</strong> —— <em>There is therefore an ongoing demand for innovations in software tools that help make software more reliable and maintainable.</em></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;生活的方方面面都依赖着高质量软件的可靠操作，但是软件的维护是代价大复杂程度高且耗时的过程，这就要求着工具的不断改进来降低软件的复杂度，同时帮助工程师更好的维护代码。</p>
<p><strong>Big Code</strong> —— <em>The scale of available data is massive.</em></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;系统源码的庞大，各类元数据(authorship, bug-fixes, reviews)的繁多，暗示了一种新的数据驱动的开发软件工具的办法：在大型且有代表性的软件集上进行统计分布估算，从而能在大多数情况下取得好的结果。</p>
<blockquote>
<p>原文中，后两个概念出现在第二部分中，但我把它们提到前面来，进行简要的阐释。如果想要更详细的内容，请阅读原论文~</p>
</blockquote>
<p><strong>The Naturalness Hypothesis</strong> —— <em>Software is a form of human communication; software corpora have similar statistical properties to natural language corpora; and these properties can be exploited to build better software engineering tools.</em></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;软件是人类交流的一种形式，软件语料库具有和自然语言语料库相似的统计特征，这些特征可以利用在建立更好的软件工具上。基于这个假设，就可以使用机器学习的方法来设计模型学习开发者们是如何写代码使用代码的。</p>
<p><strong>Code Predictability</strong> —— <em>Code is conventional, idiomatic, and familiar.</em></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;代码书写存在规律性和习惯性，因此概率统计模型可以针对代码学习出有效的特征，给出相应任务的反馈。</p>
<h2 id="Text-Code-and-Machine-Learning"><a href="#Text-Code-and-Machine-Learning" class="headerlink" title="Text, Code and Machine Learning"></a>Text, Code and Machine Learning</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;程序语言代码是人类和计算机沟通交流的媒介，尽管代码和文本间存在许多相似，但是代码对于目前存在的ML和NLP技术来说仍然是相对较新的问题领域。因此，仔细枚举出代码和文本间的差异有利于学习到通过修改现有的NLP技术来处理代码的适当方法。</p>
<ul>
<li><strong>Executability</strong></li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所有代码都是可执行的，代码通常在语义上敏感，很小的改动也会使得代码的表达产生很大的变化；而读者面对自然语言时也能理解可能错误的地方。所以代码语义对于噪声的敏感性需要将概率方法和形式方法结合起来，例如应用严格的形式限制来过滤概率模型的输出。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;另一方面，编程语言可以在多种语言中互通转换，所有的主流语言都是Turing-complete的，即将现实世界的语言移植到新的语言和平台可行但极具挑战的。现有的ML技术能达到的是在语法相近的Java语言和C#语言上完成转换。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;代码的可执行性在程序中引起控制和数据流，并且赋予代码有静态代码和动态视图（例如执行轨迹）的两种形式。  </p>
<ul>
<li><strong>Formality</strong></li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;程序设计语言是形式化的语言，自上而下地设计给使用者们。源代码在局部上模式更密集，支持被重复使用，可将常用功能归入库函数中。同时，因为程序语言需要自动的翻译为机器语言，因此保有较好的语法性，以及丰富而明确的代码结构，可以被大麦的概率模型很好地利用。</p>
<ul>
<li><strong>Cross-Channel Interation</strong></li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;代码中自然的语义单元通常有标识符、语句、代码块和函数，而这些很难被映射到文本语义单元上。相对于文本而言，代码函数的抽象语法树通常更深且有更多重复的内部结构。在代码库中，不同的任务被描述成不同的语言，且比文本有更高的新词率。</p>
<h2 id="Probablistic-Models-of-Code"><a href="#Probablistic-Models-of-Code" class="headerlink" title="Probablistic Models of Code"></a>Probablistic Models of Code</h2><blockquote>
<p>这一章节关注到许多关于源代码的概率机器学习模型，总共列举了许多上百种方法。在这里我就不展开详细介绍每一种方法，之后有时间会多介绍几个自己研究过的模型，主要会集中在第二个模型类别里，大家可以自己找感兴趣的模型学习。</p>
</blockquote>
<p>基于模型概率分布等式的形式和输入输出的形式，作者对模型进行了分类，而每一类模型下会继续细分。</p>
<h3 id="Code-generating-Models"><a href="#Code-generating-Models" class="headerlink" title="Code-generating Models"></a>Code-generating Models</h3><p>“Code-generating Models define a probability distribution over code by stochastically modeling the generation of smaller and simpler parts of code, e.g. tokens or AST nodes.”  </p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这一类模型在于预测复杂的代码结构，他们会对生成过程做一些简化的假设，并迭代地预测代码中的元素来生成一个完整的代码单元。因为代码结构的复杂度和假设的存在性，想要生成可编译可执行的代码是有难度的。  </p>
<p>从模型生成过程中所用的代码粒度来说:  </p>
<ul>
<li>Token-level Models(sequences)<br>这类模型把代码看做一组元素的序列，有序地来依次进行预测任务。序列中常用的n-gram模型在捕获局部和简单的依赖十分有效，但是对于代码来说，代码更冗长，且光靠代码上文会丢失很多信息，因此，许多方法在使用n-gram模型的基础上尝试了许多改进。当循环深度神经网络在序列问题上表现成功后，也被应用到了代码模型上，不过比n-gram模型需要更多的数据。</li>
<li>Syntactic Models(trees)</li>
<li>Semantic Models(graphs)</li>
</ul>
<p>从模型的输出目的来说:</p>
<ul>
<li>Language Models</li>
<li>Code Transducer Models</li>
<li>Multimodal Models</li>
</ul>
<h3 id="Representational-Models-of-Code"><a href="#Representational-Models-of-Code" class="headerlink" title="Representational Models of Code"></a>Representational Models of Code</h3><p>“Representational Models of Code take an abstract representation4 of code as input. Example representations include token contexts or data flow. The resulting model yields a conditional probability distribution over code element properties, like the types of variables, and can predict them.”  </p>
<h3 id="Pattern-Mining-Models"><a href="#Pattern-Mining-Models" class="headerlink" title="Pattern Mining Models"></a>Pattern Mining Models</h3><p>“Pattern Mining Models infer, without supervision, a likely latent structure within code. These models are an instantiation of clustering in the code domain; they can find reusable and human-interpretable patterns.”</p>
]]></content>
      <categories>
        <category>Big Code</category>
      </categories>
      <tags>
        <tag>paper</tag>
        <tag>Code Naturalness</tag>
      </tags>
  </entry>
  <entry>
    <title>【分布式系统_1】 文件系统</title>
    <url>/2020/08/05/Distributed-System-1/Distributed-System-1/</url>
    <content><![CDATA[<blockquote>
<p>分布式系统课件1：分布式文件系统，从文件系统入手，进一步介绍分布式文件系统。</p>
</blockquote>
<h3 id="文件系统-FS）"><a href="#文件系统-FS）" class="headerlink" title="文件系统(FS）"></a>文件系统(FS）</h3><pre><code>1. 文件系统概述  
文件系统：操作系统中负责管理和存取信息的模块  
• 基本功能：  
  1）文件的按名存取  
  2）文件目录的建立和维护  
  3）实现逻辑文件到物理文件的转换（核心）  
  4）文件存储空间的分配和管理  
  5）数据保密、保护和共享  
  6）提供一组用户使用的操作  

2. 文件与目录
• 实现按名存取的文件系统的优点：
  1）将用户从复杂的屋里存储地址管理中解放出来
  2）可方便地对文件提供各种安全、保密和保护措施
  3）实现文件的共享（同名共享、异名共享）

• 如何实现按名存取？
  当用户要求存取某个文件时，系统查找目录文件，获得对应的文件目录
  在文件目录中，根据用户给定的文件名寻找到对应文件的文件控制块（文件目录项）
  通过文件控制块所记录的该文件的相关信息依次存取该文件的内容

• 文件目录
  1）文件目录：建立和维护的关于系统的所有文件的清单
  2）文件控制块：每个目录项对应一个文件的信息描述（存取控制信息、结构信息、使用信息、管理信息）
  3）目录文件：目录信息也以文件的形式存放

• 树形目录结构

3. 文件的物理结构
• 文件在物理存储中的存放方法和组织关系
  块（物理记录）的划分、记录的排列、索引的组织、信息的搜素
• 常见的文件物理结构
  顺序文件（连续存储）；链接文件；索引文件</code></pre>
<h3 id="分布式文件系统-DFS"><a href="#分布式文件系统-DFS" class="headerlink" title="分布式文件系统(DFS)"></a>分布式文件系统(DFS)</h3><pre><code>1. 体系架构  
• DFS实现的思路：  
  1）保证每台机器均可透明地访问其他机器上的文件（通过RPC调用）  
  2）将所有机器的文件系统关联起来，形成一个对外统一的整体  
• Client-Server Architectures：文件系统的挂载  
• Symmetric Architectures：通过特殊的hash算法将文件划分到各台机器上，需要访问文件时可根据hash算法进行定位
• Cluster-Based Distributed File Systems  
  主节点进行管理，从节点存储数据  
  文件切分成块，分散存储在从节点上  

2. 文件访问  
• 单机多进程访问同一文件：读写锁  
• 不同机器上进程访问同一文件  
• 注意临界区问题  

3. 备份与一致性  
• 客户端备份：Client-Server DFS  
• 服务器端备份：Cluster-Based DFS  

4. 容错管理  </code></pre>
<h3 id="Hadoop分布式文件系统-HDFS"><a href="#Hadoop分布式文件系统-HDFS" class="headerlink" title="Hadoop分布式文件系统(HDFS)"></a>Hadoop分布式文件系统(HDFS)</h3><pre><code>1. 设计考量
• HDFS：Hadoop Distributed File System 分布式文件系统
• MapReduce：并行计算框架
• 文件由数据块集合组成，每个数据块在本地文件系统中以单独文件进行存储

2. 体系结构</code></pre>
<p><img src="/img/202008052.png"></p>
<pre><code>• NameNode(masters)：每个集群一个名字节点，负责文件系统元数据操作、数据块的复制和定位
核心数据文件包括：元数据镜像文件、操作日志文件；元数据保存在内存中
• SecondaryNameNode(backups)：NameNode的备份节点 （防止Log过大回恢复时间过长 冷备份/离线）
“检查点”：定期从NameNode上下载镜像和日志，合并成新的，在本地保存，并写回NameNode
• DataNodes(slaves)：集群中每个节点一个数据节点，负责数据块的存储；为客户端提供实际文件数据
HDFS默认Block大小是64MB；若一个文件小于一个数据块的大小，并不占用整个数据块存储空间

3. 文件访问
• 文件写入HDFS
  NameNode告知客户端文件的每一个数据块存储在何处，客户端将数据块直接传输到指定的数据节点
• 数据存放策略（目标：负载均衡，快速访问，容错）
  三个副本：当前DataNode（快速写入）、不同机架（减少跨rack的网络流量）、相同机架其他节点（应对交换机故障）
  若有更多副本，随机节点
• 数据读取策略：从NameNode获得数据块不同副本的存放位置列表，最近者优先原则
• 文件访问模型：”一次写入多次读取“，不允许更改，仅容许追加；修改内容需删除重新写入;对于单文件只
               支持并发读，不支持并发写
  好处：避免读写冲突、无需文件锁

4. 备份与一致性
• HDFS数据备份优点
  1）加快数据传输速度
  2）容易检查数据错误
  3）保证数据的可靠性
• 一个文件有若干备份，写入成功的备份之间是强一致的

5. 容错机制
• DataNode故障：宕机，节点上所有的数据都会标记为不可读
• 定期检查备份因子：NameNode侦测DataNode故障，数据块自动复制到剩余的节点以保证满足备份因子
• NameNode故障：根据SecondaryNameNode中得FsImage和Editlog数据进行恢复</code></pre>
]]></content>
      <categories>
        <category>系统</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>笔记整理</tag>
      </tags>
  </entry>
  <entry>
    <title>【分布式系统_2】 MapReduce</title>
    <url>/2020/08/08/Distributed-System-2/Distributed-System-2/</url>
    <content><![CDATA[<h2 id="MapReduce批处理系统"><a href="#MapReduce批处理系统" class="headerlink" title="MapReduce批处理系统"></a>MapReduce批处理系统</h2><ol>
<li>Hadoop简介 —— Apache下的一个开源分布式计算平台<br> 基于Java语言开发，具有很好的跨平台特性  <ul>
<li>核心：分布式文件系统HDFS 和 MapReduce  </li>
<li>发展简史：Hadoop is from Yahoo.  </li>
<li>MapReduce在Hadoop中的位置：MapReduce是对并行计算的封装，将一个大的运算任务分解到集群的每个节点上，充分运用集群资源，缩短运行时间。</li>
</ul>
</li>
</ol>
<p><img src="/2020/08/08/Distributed-System-2/Distributed-System-2/202008081.png">  </p>
<ol start="2">
<li><p>体系结构</p>
<ul>
<li>Client<br>提交作业：用户编写的MapReduce程序通过Client提交到JobTracker端<br>作业监控：用户可通过Client提供一些接口查看作业的运行状态  </li>
<li>JobTracker<br>资源管理：监控TaskTracker与Job的状况（一旦发现失败，就将Task转移到其他节点）<br>作业调度：将Job拆分成Task，跟踪Task的执行进度、资源使用量等信息，由TaskScheduler调度  </li>
<li>TaskTracker<br>执行操作：接收JobTracker发送过来的命令并执行（如启动新Task、杀死Task等）<br>划分资源：使用“Slot”等量划分本节点上的资源量（CPU 、内存等），一个Task获取到一个Slot后才有机会运行<br>汇报信息：通过“心跳”将本节点上的资源使用情况和任务运行进度汇报给JobTracker  </li>
<li>Task进程<br>执行任务：Map Task &amp; Reduce Task;Jar包发送到TaskTracker，利用反射和代理机制动态加载代码  </li>
</ul>
</li>
<li><p>工作流程<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>map阶段 - shuffle阶段 - reduce阶段</strong>  </p>
</li>
</ol>
<ul>
<li>从节点角度看流程<br><img src="/2020/08/08/Distributed-System-2/Distributed-System-2/202008082.png">  </li>
</ul>
<ol>
<li>InputFormat: 定义了怎样与物理存储之间的映射，理想的分片大小是一个HDFS块  </li>
<li>Split: 逻辑概念，包含一些元数据信息。划分方法由用户决定  </li>
<li>Map: Hadoop为每个split创建一个Map任务，执行Map函数;Map任务数量：由Split的多少决定  </li>
<li>Shuffle:   <ul>
<li>Map端：写入缓存，溢写（分区、排序、合并_Combine）(局部), Key值相同的记录拼接在一起归并_Merge成文件到本地（一个Map节点的归并成一个文件）  </li>
<li>Reduce端: 领取数据，归并数据到缓存、本地（多个溢写文件的归并），数据输入给Reduce任务  </li>
</ul>
</li>
</ol>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5) Reduce:  执行Reduce函数;Reduce任务数量: 一般略小于reduce slot的数目，预留一些系统资源处理可能发生的错误<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6) OutputFormat: 与InputFormat对应  </p>
<ul>
<li>Hadoop序列化<br>  序列化：指把结构化对象转化为字节流以便在网络上传输或在磁盘上永久存储的过程<br>  序列化格式特点：紧凑、快速、可扩展、互操作<br>  Hadoop的序列化格式：Writable （数据类型一般为LongWritable、Text、IntWritable…）</li>
</ul>
<ol start="4">
<li>容错机制</li>
</ol>
<ul>
<li>代码错误  </li>
<li>机器故障<br>  1）Task容错：Map Task重新去HDFS读入数据，重新执行Map任务；Reduce Task重新去本地磁盘读入数据，重新执行Reduce任务<br>  2）TaskTracker容错：JobTracker接受不到“心跳”，安排其他TaskTracker重新运行<br>  3）JobTracker容错：单点故障，所有任务重新运行  </li>
</ul>
<h2 id="MapReduce编程"><a href="#MapReduce编程" class="headerlink" title="MapReduce编程"></a>MapReduce编程</h2><pre><code>1. 单个MapReduce
• 单元运算——WordCount程序任务
1）Map函数：处理输入、分词、组合键值对
   Map(K, V) &#123; For each word w in V :  Collect(w,1) &#125;
2）Combine函数：可选，Map内部先行合并
3）Reduce函数：按分区求和，处理输出
   Reduce的输入数据为&lt;Key,Iterable容器&gt;
   Reduce(K, V[ ]) &#123; For each v in V : count += v;    Collect(K, count); &#125;
4）Main函数：Job

• 二元运算（Join、集合交集、集合并集）
1）关系的自然连接（作业1-3）
   Map过程需要标记来自哪个关系表，Key值为连接属性。
   循环遍历 or 哈希实现

2. 组合式MapReduce
例：词频统计后，按词频范围划分组
划分为若干子任务，连续执行多个MapReduce，存在依赖关系（前一个Job做完后才能做下一个Job）
• 隐式依赖描述：定死顺序，且未考虑容错 
               runJob1(input , tmp);   runJob2(tmp, output);
• 显示依赖描述：调度灵活 更优
               Job3.addDepending(Job1)；设置依赖关系
               JobControl.addJob(Job1);     把多个job加入jobcontrol中

3. 链式MapReduce
例：词频统计后过滤掉词频较高的，且不修改词频统计程序
• 规则
整个Job中只能有一个Reducer，Reducer前面一个or多个Mapper，Reducer后面0个or多个Mapper
• 写法
ChainMapper.addMapper()   ChainReducer.setReducer()  ChainReducer.addMapper()

4. 迭代式MapReduce（Kmeans、PageRank）
• 迭代式任务的特征：
整个任务由一系列的子任务循环构成；子任务的执行操作完全相同；一个子任务的输出是下一个子任务的输入；
一个子任务是一个MapReduce Job
• 写法
While() &#123;   runIteration（） iter++； &#125;
• 性能瓶颈
每一迭代步结束时将结果写入HDFS，下一步将该结果再次从HDFS读出；
Map -&gt; 本地磁盘 -&gt; Reduce （I/O浪费、存储浪费）

5. Distributed Cache 
大表连接小表，将小表广播出去（小数据量-&gt;广播 ； 大数据量-&gt;本地，减少数据移动）</code></pre>
]]></content>
      <categories>
        <category>系统</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>笔记整理</tag>
      </tags>
  </entry>
</search>
